# -*- coding: utf-8 -*-
"""xgboost_hyperparameter_tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dv3hYDzOdWCJNvhPBIghHbOX-Vfw0WFp
"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import GradientBoostingClassifier

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer 
import matplotlib.pyplot as plt
import numpy as np
from sklearn import svm

support=open("support.txt","r").readlines()
bully=open("bully.txt","r").readlines()

print("Support labelled",len(support))
print("bully labelled",len(bully))

label=[]
comm=[]
for i in bully:
  label.append('bullying')
  comm.append(i[:-1].lower())

for i in support:
  label.append('support')
  comm.append(i[:-1].lower())
df = pd.DataFrame({'sentence':comm, 'label':label})
print(df)
print(comm)

f=open("features.txt","r").readlines()
features=[]
for i in f:
  features.append(i[:-1].lower())

##TfidfVectorizer

tfidfconverter = TfidfVectorizer(max_features=1000,vocabulary=features,ngram_range=(1, 3))  
X = tfidfconverter.fit_transform(comm).toarray()
X[0]

##Features extraction
print(len(tfidfconverter.get_feature_names()))
tfidfconverter.get_feature_names()
tfidfconverter.vocabulary_

idf=tfidfconverter.idf_
rr=dict(zip(tfidfconverter.get_feature_names(), idf))
pd.DataFrame(rr.items())

feature_names = np.array(tfidfconverter.get_feature_names())
sorted_by_idf = np.argsort(tfidfconverter.idf_)
print("Features with lowest idf:\n{}".format(
       feature_names[sorted_by_idf[:3]]))

from sklearn.model_selection import train_test_split  
X_train, X_test, y_train, y_test = train_test_split(X, label, test_size=0.2, random_state=0,stratify=label)


params={
 "learning_rate"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,
 "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],
 "min_child_weight" : [ 1, 3, 5, 7 ],
 "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
 "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ]
    
}

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
import xgboost
from xgboost import XGBClassifier
classifier=xgboost.XGBClassifier()

random_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=50,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)
print("Hello")
random_search.fit(X_train, y_train)
random_search.best_estimator_

xgb_clf=xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0.1,
              learning_rate=0.2, max_delta_step=0, max_depth=2,
              min_child_weight=1, missing=None, n_estimators=50, n_jobs=1,
              nthread=None, objective='binary:logistic', random_state=0,
              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
              silent=None, subsample=1, verbosity=1)
xgb_clf.fit(X_train,y_train)

predictions = xgb_clf.predict(X_test)
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(confusion_matrix(y_test,predictions))  
print(classification_report(y_test,predictions))  
print(accuracy_score(y_test, predictions))

input=['poli annu mwone','nee ninte vettil poyy para thayoli']
o=tfidfconverter.fit_transform(input).toarray()
predict=xgb_clf.predict(o)
print(predict)